<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Sign Language Input - Mediapipe</title>
  <style>
    body {
      margin: 0;
      text-align: center;
      background-color: #111;
      color: #fff;
    }
    video, canvas {
      position: absolute;
      left: 50%;
      transform: translateX(-50%);
    }
    canvas {
      z-index: 1;
    }
    h1 {
      margin-top: 20px;
      font-family: sans-serif;
    }
    #gesture-label {
  font-family: monospace;
  font-size: 24px;
  margin-top: 500px;
}

  </style>
</head>
<body>
  <h1>âœ‹ Sign Language Input</h1>
  <video id="webcam" autoplay playsinline width="640" height="480"></video>
  <canvas id="output" width="640" height="480"></canvas>
  <h2 id="gesture-label">Gesture: <span id="label">Detecting...</span></h2>

  <!-- Mediapipe & Drawing Library -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.min.js"></script>

  <script>
    const videoElement = document.getElementById('webcam');
    const canvasElement = document.getElementById('output');
    const canvasCtx = canvasElement.getContext('2d');

    // Set up Mediapipe Hands
    const hands = new Hands({
      locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`
    });

    hands.setOptions({
      maxNumHands: 1,
      modelComplexity: 1,
      minDetectionConfidence: 0.7,
      minTrackingConfidence: 0.7
    });

    hands.onResults((results) => {
      canvasCtx.save();
      canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
      canvasCtx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);

      if (results.multiHandLandmarks) {
        for (const landmarks of results.multiHandLandmarks) {
          drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS, { color: '#00FF00', lineWidth: 3 });
          drawLandmarks(canvasCtx, landmarks, { color: '#FF0000', radius: 5 });

          // Log landmark data
          console.log("Landmark positions:", landmarks);

          // Process and display gesture
          const gesture = detectGesture(landmarks);
          document.getElementById("label").textContent = gesture;
        }
      }
      canvasCtx.restore();
    });

    // Helper: check if a finger is folded
    function isFingerFolded(landmarks, tipIndex, baseIndex) {
      return landmarks[tipIndex].y > landmarks[baseIndex].y;
    }

    // Helper: check if a finger is extended
    function isFingerExtended(landmarks, tipIndex, baseIndex) {
      return landmarks[tipIndex].y < landmarks[baseIndex].y;
    }

    // Detect gesture based on landmarks
    function detectGesture(landmarks) {
      const thumbIsFolded = isFingerFolded(landmarks, 4, 2);
      const indexIsFolded = isFingerFolded(landmarks, 8, 6);
      const middleIsFolded = isFingerFolded(landmarks, 12, 10);
      const ringIsFolded = isFingerFolded(landmarks, 16, 14);
      const pinkyIsFolded = isFingerFolded(landmarks, 20, 18);

      const indexIsExtended = isFingerExtended(landmarks, 8, 6);
      const othersFolded = middleIsFolded && ringIsFolded && pinkyIsFolded;

      if (thumbIsFolded && indexIsFolded && middleIsFolded && ringIsFolded && pinkyIsFolded) {
        return "A âœŠ";
      } else if (!thumbIsFolded && !indexIsFolded && !middleIsFolded && !ringIsFolded && !pinkyIsFolded) {
        return "Open âœ‹";
      } else if (indexIsExtended && othersFolded) {
        return "Point ðŸ‘‰";
      } else {
        return "Unknown";
      }
    }

    // Use camera from Mediapipe Utils
    const camera = new Camera(videoElement, {
      onFrame: async () => {
        await hands.send({ image: videoElement });
      },
      width: 640,
      height: 480
    });
    camera.start();
  </script>
</body>
</html>
